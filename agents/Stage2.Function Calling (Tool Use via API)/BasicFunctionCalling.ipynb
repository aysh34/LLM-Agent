{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b538cc",
   "metadata": {},
   "source": [
    "## Stage 2: Tool Use / Function Calling\n",
    "\n",
    "It's time to give agent the ability to call real-world functions like fetching data or executing code when needed, not just describe what it would do.\n",
    "\n",
    "### Concept Behind Tool Use:\n",
    "\n",
    "LLMs are good at reasoning, but they can't directly:\n",
    "\n",
    "- access APIs\n",
    "\n",
    "- run Python\n",
    "\n",
    "- browse the web, unless we build that in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b0e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install together python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55536503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e49c61-3035-4bd6-853e-f0da74985700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call: add({'a': 5, 'b': 7}) = 12\n",
      "Tool call: add({'a': 5, 'b': 7}) = 12\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 0.6 queries and 45000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, msg\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m run_agent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 5 plus 7?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mrun_agent\u001b[1;34m(goal, max_steps)\u001b[0m\n\u001b[0;32m     11\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Call the LLM\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     15\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.3-70B-Instruct-Turbo-Free\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \n\u001b[0;32m     16\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful agent.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     17\u001b[0m                   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: goal},\n\u001b[0;32m     18\u001b[0m                   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: memory}],\n\u001b[0;32m     19\u001b[0m         tools\u001b[38;5;241m=\u001b[39m[{\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     22\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdds two numbers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     25\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     27\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     28\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     29\u001b[0m                     },\n\u001b[0;32m     30\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m                 }\n\u001b[0;32m     32\u001b[0m             }\n\u001b[0;32m     33\u001b[0m         }]\n\u001b[0;32m     34\u001b[0m     )\n\u001b[0;32m     36\u001b[0m     msg \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# If it's a tool call\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\Lib\\site-packages\\together\\resources\\chat\\completions.py:141\u001b[0m, in \u001b[0;36mChatCompletions.create\u001b[1;34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m    113\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    116\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ChatCompletionRequest(\n\u001b[0;32m    117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    118\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    139\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 141\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    142\u001b[0m     options\u001b[38;5;241m=\u001b[39mTogetherRequest(\n\u001b[0;32m    143\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    145\u001b[0m         params\u001b[38;5;241m=\u001b[39mparameter_payload,\n\u001b[0;32m    146\u001b[0m     ),\n\u001b[0;32m    147\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "File \u001b[1;32mD:\\Anaconda3\\Lib\\site-packages\\together\\abstract\\api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    241\u001b[0m ]:\n\u001b[0;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[0;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[1;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mD:\\Anaconda3\\Lib\\site-packages\\together\\abstract\\api_requestor.py:635\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m     content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    636\u001b[0m         content,\n\u001b[0;32m    637\u001b[0m         result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    638\u001b[0m         result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    639\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    640\u001b[0m     ),\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    642\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda3\\Lib\\site-packages\\together\\abstract\\api_requestor.py:707\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 0.6 queries and 45000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "load_dotenv()\n",
    "client = Together()\n",
    "\n",
    "# step 1 - Define a Tool\n",
    "tools = {\n",
    "    \"add\": lambda a, b: a + b\n",
    "}\n",
    "\n",
    "def run_agent(goal, max_steps=5):\n",
    "    memory = \"\"\n",
    "    for step in range(max_steps):\n",
    "        # Call the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",   \n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "                      {\"role\": \"user\", \"content\": goal},\n",
    "                      {\"role\": \"assistant\", \"content\": memory}],\n",
    "            tools=[{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"add\",\n",
    "                    \"description\": \"Adds two numbers\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"a\": {\"type\": \"number\"},\n",
    "                            \"b\": {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\"a\", \"b\"]\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        )\n",
    "\n",
    "        msg = response.choices[0].message\n",
    "\n",
    "        # If it's a tool call\n",
    "        if msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                fn = tc.function.name\n",
    "                args = eval(tc.function.arguments)\n",
    "                result = tools[fn](**args)\n",
    "\n",
    "                # feed result back to LLM\n",
    "                memory += f\"\\nTool {fn} returned: {result}\"\n",
    "                print(f\"Tool call: {fn}({args}) = {result}\")\n",
    "        else:\n",
    "            print(\"Final Answer:\", msg.content)\n",
    "            break\n",
    "\n",
    "run_agent(\"What is 5 plus 7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df1a10-9ac0-4b54-996c-e2bbb323eba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
